{
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "1.3 - Missing dates imputation - Modified",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  },
  "lastEditStatus": {
   "notebookId": "6k3jqgtsfi7ycdv3icxw",
   "authorId": "6771236268325",
   "authorName": "HSUNDARARAJAN",
   "authorEmail": "harish.sundarara@purposebrands.com",
   "sessionId": "38c3fa1a-af2a-445b-a03d-e2a25d78b773",
   "lastEditTime": 1766140611006
  }
 },
 "nbformat_minor": 0,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b58c4344-3af1-4279-b8b0-17bdac2e98ce",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "name": "cell1"
   },
   "source": [
    "# 1. Objective\n",
    "   The Objective of the notebook is to impute missing dates in-between the start and end date of each time series and leave the values of DV, IDVs to be null which can be treated in the missing value treatment phase."
   ],
   "id": "ce110000-1111-2222-3333-ffffff000000"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "19dadade-dea7-4242-9e32-94f0077266ad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "name": "cell2"
   },
   "source": [
    "# 2. Imports"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000001"
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e4d07d5a-140a-44c1-a35c-489383e4b0fa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "name": "cell3",
    "language": "python"
   },
   "outputs": [],
   "source": "import time\nimport os\n\nimport numpy as np\nimport pandas as pd\n\nfrom datetime import datetime\nimport shutil\n\n# We can also use Snowpark for our analyses!\nfrom snowflake.snowpark.context import get_active_session\nfrom snowflake.snowpark import functions as F, types as T\nsession = get_active_session()",
   "id": "ce110000-1111-2222-3333-ffffff000002"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0c77c924-36fc-440a-887f-51237a44a936",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "name": "cell4",
    "collapsed": false
   },
   "source": [
    "# 3. Setup environment"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000003"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "990ccc4f-afc5-4419-b333-14133072a82e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "name": "cell5"
   },
   "source": [
    "## 3.1. Load Config"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000004"
  },
  {
   "cell_type": "code",
   "id": "267122ce-1f8c-48fa-a6c1-b8176d4e4f8e",
   "metadata": {
    "language": "python",
    "name": "cell34"
   },
   "outputs": [],
   "source": [
        "import yaml\n",
        "stage_path = \"@ORANGE_ZONE_SBX_TA.PUBLIC.CONNECTIONS/config_new_PROD.yaml\"\n",
        "stream = session.file.get_stream(stage_path)\n",
        "yaml_text = stream.read().decode()\n",
        "app_config = yaml.safe_load(yaml_text)"
      ],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "6390d7da-5bc5-4bed-8bef-24a09221ad59",
   "metadata": {
    "name": "cell7",
    "collapsed": false
   },
   "source": "## 3.2 Update Output Database, Schema , table"
  },
  {
   "cell_type": "code",
   "id": "76093b06-022d-4330-a9a5-b6240f5b5c73",
   "metadata": {
    "language": "python",
    "name": "cell40"
   },
   "outputs": [],
   "source": "output_database = app_config[\"general_inputs\"][\"output_database\"]\noutput_schema = app_config[\"general_inputs\"][\"output_schema\"]\nprint(output_database, output_schema)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "4e9ed80e-bcef-4a00-a647-6fc23f120fed",
   "metadata": {
    "language": "python",
    "name": "cell41"
   },
   "outputs": [],
   "source": "session.use_database(output_database)\nsession.use_schema(output_schema)\noutput_table_name = \"PROD_MISSING_DATE_TREATMENT_OUTPUT\"",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "8b9de86c-fc20-4b3b-a78d-2812a9b7d640",
   "metadata": {
    "language": "python",
    "name": "cell42"
   },
   "outputs": [],
   "source": "# Example check (optional)\nprint(\"✅ Snowpark Session Initialized Successfully!\")\nprint(\"Current Database:\", session.get_current_database())\nprint(\"Current Schema:\", session.get_current_schema())",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "152ae8e2-d27c-4ed8-af4a-b1152e47389d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "name": "cell9",
    "collapsed": false
   },
   "source": [
    "## 3.2. Capturing necessary variables"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000008"
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "983dc824-f59a-4bed-a273-ce096afb202b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "name": "cell10",
    "language": "python"
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/markdown": [
       "# Algorithms"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/markdown": [
       "# Future Forecast"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/markdown": [
       "# Creating `app_config`\n",
       "\n",
       "Manual intervention not needed from here"
      ]
     },
     "metadata": {}
    }
   ],
   "source": "# Get the modeling granularity\nmodeling_granularity_conf = app_config[\"general_inputs\"][\"modeling_granularity\"]\n\n# Get date and Dependent variable\ndv_config = app_config[\"general_inputs\"][\"dependent_variable\"]\nds_config = app_config[\"general_inputs\"][\"date_var\"]",
   "id": "ce110000-1111-2222-3333-ffffff000009"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "29469053-0161-49d1-8976-93fbe6055f12",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "name": "cell13",
    "collapsed": false
   },
   "source": [
    "# 4. Utility Functions"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000012"
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4d2034b4-85f4-4fab-92ac-76973e7baf90",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "name": "cell20",
    "language": "python"
   },
   "outputs": [],
   "source": "broadcast_date_col = ds_config\nbroadcast_granularity = modeling_granularity_conf\nbroadcast_algo_params = app_config['data_processing']['missing_value_treatment']",
   "id": "ce110000-1111-2222-3333-ffffff000019"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "241a5947-693e-4b7a-9ae1-9426a4565e0c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "name": "cell21",
    "collapsed": false
   },
   "source": [
    "# 5. Load Data"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000020"
  },
  {
   "cell_type": "code",
   "id": "a90b56e2-3bd8-49cd-8949-fe135af93708",
   "metadata": {
    "language": "python",
    "name": "cell14"
   },
   "outputs": [],
   "source": "df = session.table(\"ORANGE_ZONE_SBX_TA.PUBLIC.PROD_ADS_STABLE_V4\")\ndf = df.withColumn(ds_config, F.col(ds_config).cast(\"timestamp\"))",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "838de770-dbc9-43bd-8fba-0cfc8a5ddf4a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "name": "cell23",
    "collapsed": false
   },
   "source": [
    "# 6. Identify missing dates\n",
    "\n",
    "The aim is to create a dataframe that has all the dates supposed to be there. For the missing dates, if any, it would have nulls in the other columns. This dataframe would be passed through the missing value treatment phase. That would ensure that we have effectively tackled mising dates issue."
   ],
   "id": "ce110000-1111-2222-3333-ffffff000022"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "82203c1f-750a-4eb5-869a-1ee0a2d75dc1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "name": "cell24"
   },
   "source": [
    "## 6.1. Generate granularity-wise start and end dates"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000023"
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b4a8e194-ba07-4c8c-b216-c91f4a0e63d1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "name": "cell25",
    "language": "python"
   },
   "outputs": [],
   "source": "# Finding start and end dates for each granularity\nstart_end_date_df = (\n    df\n    .group_by(modeling_granularity_conf)\n    .agg(\n        F.min(ds_config).alias('start_date'),\n        F.max(ds_config).alias('end_date')\n    )\n)",
   "id": "ce110000-1111-2222-3333-ffffff000024"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e5ffb9fe-32c5-4828-9d57-8219fb97466a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "name": "cell26",
    "collapsed": false
   },
   "source": [
    "## 6.2. Infer frequency of dates in data"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000025"
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e1244360-a35b-438b-990c-1fae5adc95a1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "name": "cell27",
    "language": "python"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Inferred frequency: W-MON\n"
     ]
    }
   ],
   "source": [
    "few_dates = (\n",
    "    df.\n",
    "    select(ds_config)\n",
    "    .distinct()\n",
    "    .toPandas()[ds_config]\n",
    "    .sort_values(ascending=True)\n",
    "    .to_numpy()[-5:]\n",
    ")\n",
    "\n",
    "date_frequency = pd.infer_freq(few_dates)\n",
    "\n",
    "print(f\"Inferred frequency: {date_frequency}\")"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000026"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "66d41e68-6ca0-4a2a-b7c2-b4e9be28f5af",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "name": "cell28",
    "collapsed": false
   },
   "source": [
    "## 6.3. Generate output dataset with all dates\n",
    "\n",
    "This dataset would have nulls for the other columns wherever there is a missing date."
   ],
   "id": "ce110000-1111-2222-3333-ffffff000027"
  },
  {
   "cell_type": "code",
   "id": "8ac409d6-0f8c-440a-bd71-d8abc2d6e205",
   "metadata": {
    "language": "python",
    "name": "cell35"
   },
   "outputs": [],
   "source": "\n\n# 1️⃣ Define the Python function\ndef generate_dates(start_date, end_date, freq='D'):\n    if start_date is None or end_date is None:\n        return []\n    return [d.date() for d in pd.date_range(start=start_date, end=end_date, freq=freq)]\n\n# 2️⃣ Register the function as a Snowpark UDF\ngenerate_dates_udf = F.udf(\n    func=lambda start, end: generate_dates(start, end, date_frequency),\n    return_type=T.ArrayType(T.DateType()),\n    input_types=[T.DateType(), T.DateType()]\n)\n\n# 3️⃣ Apply the UDF to generate the list of dates\nstart_end_date_df = start_end_date_df.with_column(\n    \"dates_range\",\n    generate_dates_udf(F.col(\"start_date\"), F.col(\"end_date\"))\n)\n\n# 4️⃣ Explode the list into separate rows\ndf_dates = (\n    start_end_date_df\n    .with_column(ds_config, F.explode(F.col(\"dates_range\")))\n    .select(*modeling_granularity_conf, F.col(ds_config))\n)\ndf_dates = df_dates.withColumn(ds_config, F.col(ds_config).cast(\"timestamp\"))\n# Show the resulting DataFrame\nexpected_no_of_dates = df_dates.count()\nactual_no_of_dates = df.count()\nif expected_no_of_dates != actual_no_of_dates:\n    print(f\"Expected no of dates: {expected_no_of_dates}\")\n    print(f\"Actual no of dates: {actual_no_of_dates}\")\n    print(f\"Dates in data are not as per the expectation. So, Imputation of missing dates is performed!!!\")\nelse:\n    print(f\"There are no missing dates in between start and end date for each combination. Dates in data as expected\")",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "3cfd47d6-4936-44ac-8a2c-84545be3ef6f",
   "metadata": {
    "language": "python",
    "name": "cell37",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "start_end_date_df",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "042830b7-5baf-41fa-af70-6860ea0d5376",
   "metadata": {
    "language": "python",
    "name": "cell36",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "df_dates",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "df0d1f50-9589-4a70-83dc-b7e3831f7cd8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "name": "cell30",
    "language": "python",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": [
    "df = df.join(df_dates, on=[*modeling_granularity_conf, ds_config], how=\"right\")"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000029"
  },
  {
   "cell_type": "code",
   "id": "2d846156-1a72-4604-8e7c-800f85500954",
   "metadata": {
    "language": "python",
    "name": "cell38"
   },
   "outputs": [],
   "source": "df.count()",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d9a50dfc-203b-4988-abcf-774ba857f7f5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "name": "cell31",
    "collapsed": false
   },
   "source": [
    "# 7. Store output"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000030"
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7d055c2d-97a6-4e1d-b577-4e61af6c2356",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "name": "cell33",
    "language": "python"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "File stored successfully.\n"
     ]
    }
   ],
   "source": "results_s_with_ts = df.with_column(\"LOAD_TS\", F.current_timestamp())\nresults_s_with_ts.write.mode(\"overwrite\").save_as_table(output_table_name)\nprint(results_s_with_ts.count())",
   "id": "ce110000-1111-2222-3333-ffffff000032"
  },
  {
   "cell_type": "code",
   "id": "929ba309-d757-45e3-9e0d-ff5cc2410f42",
   "metadata": {
    "language": "python",
    "name": "cell39",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "test_df = session.table(output_table_name)\nprint(\"All data ->\", test_df.count())\nlatest_ts = test_df.select(F.max(\"LOAD_TS\")).collect()[0][0]\nlatest_data = test_df.filter(F.col(\"LOAD_TS\") == F.lit(latest_ts))\nprint(\"Latest data ->\", latest_data.count())",
   "execution_count": null
  }
 ]
}
