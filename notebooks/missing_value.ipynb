{
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "1.4 - Missing value treatment - Modified",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  },
  "lastEditStatus": {
   "notebookId": "es3dfphwolpx7zlhfmak",
   "authorId": "6771236268325",
   "authorName": "HSUNDARARAJAN",
   "authorEmail": "harish.sundarara@purposebrands.com",
   "sessionId": "c72d5c69-eb96-4bf7-b026-179360e0a495",
   "lastEditTime": 1766141045509
  }
 },
 "nbformat_minor": 0,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ae5c2f43-01ed-49d7-a2e7-00fa98b4cea3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "name": "cell1",
    "collapsed": false
   },
   "source": "# 1. Objective\n   The Objective of the notebook is to perform missing value treatment for the features mentioned in the config under \"missing value treatment\" section. The notebook applies different missing value treatment techniques to the specified features based on the config parameters",
   "id": "ce110000-1111-2222-3333-ffffff000000"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "420807b2-0d1c-4c05-afc3-1ad56d3ab7d0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "name": "cell2",
    "collapsed": false
   },
   "source": "# 2. Imports",
   "id": "ce110000-1111-2222-3333-ffffff000001"
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f770e108-5106-4fbf-9d9a-b0c2f2f68549",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "name": "cell3",
    "language": "python"
   },
   "outputs": [],
   "source": "import yaml\nimport glob\nimport numpy as np\nimport pandas as pd\nfrom distutils.command.config import config\nfrom datetime import datetime\nimport os\nimport shutil\n\n\nfrom snowflake.snowpark.context import get_active_session\nfrom snowflake.snowpark import functions as F\nsession = get_active_session()",
   "id": "ce110000-1111-2222-3333-ffffff000002"
  },
  {
   "cell_type": "code",
   "id": "b7694181-f491-4ecf-bc5c-9f66bae0af21",
   "metadata": {
    "language": "python",
    "name": "cell63"
   },
   "outputs": [],
   "source": "# Check database and schema\nprint(\"✅ Snowpark Session Initialized Successfully!\")\nprint(\"Current Database:\", session.get_current_database())\nprint(\"Current Schema:\", session.get_current_schema())",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6ecca14d-5a1b-4b96-b845-cfabf58f3633",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "name": "cell4",
    "collapsed": false
   },
   "source": [
    "# 3. Setup environment"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000003"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "62bba9c3-594b-42f0-9a82-8d7550e272fa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "name": "cell5",
    "collapsed": false
   },
   "source": "## 3.1. Load Config\nCurrently, we are reading the uploaded config file in each notebook separately. Eventually, we will have the config in a common stage and load from there so that there is one config file at the end",
   "id": "ce110000-1111-2222-3333-ffffff000004"
  },
  {
   "cell_type": "code",
   "id": "e105f8f6-09f3-4fa8-9f17-8cc52d6bdb09",
   "metadata": {
    "language": "python",
    "name": "cell47",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": [
        "import yaml\n",
        "stage_path = \"@ORANGE_ZONE_SBX_TA.PUBLIC.CONNECTIONS/config_new_PROD.yaml\"\n",
        "stream = session.file.get_stream(stage_path)\n",
        "yaml_text = stream.read().decode()\n",
        "app_config = yaml.safe_load(yaml_text)"
      ],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "a94d1319-707b-4dee-9f2b-36dd2b615922",
   "metadata": {
    "name": "cell61",
    "collapsed": false
   },
   "source": "### 3.2 Update Output Database, Schema , table"
  },
  {
   "cell_type": "code",
   "id": "7e5fb944-32c3-4fd1-8cea-b2b217579825",
   "metadata": {
    "language": "python",
    "name": "cell48",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "output_database = app_config[\"general_inputs\"][\"output_database\"]\noutput_schema = app_config[\"general_inputs\"][\"output_schema\"]\nprint(output_database, output_schema)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "8cb823c5-9bd0-4389-9460-538b81061bd8",
   "metadata": {
    "language": "python",
    "name": "cell7"
   },
   "outputs": [],
   "source": "session.use_database(output_database)\nsession.use_schema(output_schema)\noutput_table_name = \"PROD_MISSING_VALUE_TREATMENT_OUTPUT\"",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "56b48fb8-18b0-4797-a629-03819922afa5",
   "metadata": {
    "language": "python",
    "name": "cell10"
   },
   "outputs": [],
   "source": "# Example check (optional)\nprint(\"✅ Snowpark Session Initialized Successfully!\")\nprint(\"Current Database:\", session.get_current_database())\nprint(\"Current Schema:\", session.get_current_schema())",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "270962f4-01bc-4423-9c0c-a34faf45fa35",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "name": "cell8",
    "collapsed": false
   },
   "source": "## 3.3. Capturing necessary variables",
   "id": "ce110000-1111-2222-3333-ffffff000007"
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "639bdfb9-488f-4f86-b0ca-8df59a49c521",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "name": "cell9",
    "language": "python"
   },
   "outputs": [],
   "source": "# Get the modeling granularity\nmodeling_granularity_conf = app_config[\"general_inputs\"][\"modeling_granularity\"]\n\n# Get date and Dependent variable\ndv_config = app_config[\"general_inputs\"][\"dependent_variable\"]\nds_config = app_config[\"general_inputs\"][\"date_var\"]",
   "id": "ce110000-1111-2222-3333-ffffff000008"
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f82db495-06b9-427c-a530-427816b8bcbe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "name": "cell12",
    "language": "python",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "broadcast_date_col = ds_config\nbroadcast_granularity = modeling_granularity_conf\nbroadcast_algo_params = app_config['data_processing']['missing_value_treatment']",
   "id": "ce110000-1111-2222-3333-ffffff000011"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "78d6ca2b-5ab0-4d3a-b2b3-d62c72dc4f1f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "name": "cell15",
    "collapsed": false
   },
   "source": [
    "# 4. Utility Functions"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000014"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2b07d110-68ed-4dc1-85a6-94d59f1f268d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "name": "cell20",
    "collapsed": false
   },
   "source": "## 4.1. Function - Implement the `mean_across_years` of missing value imputation.\n\nThis function is called from the UDF.",
   "id": "ce110000-1111-2222-3333-ffffff000019"
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7d062bcf-4572-4bd8-b98f-a656c63fd432",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "name": "cell21",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "def mean_across_years(\n",
    "    df: pd.DataFrame,\n",
    "    date_col: str,\n",
    "    numeric_cols: list,\n",
    "    modeling_granularity: list,\n",
    "    time_granularity: str = \"weekly\",\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Function to find the mean across years for different time granularities to impute for missing values\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        The dataframe which contains values for all the variables\n",
    "    date_col : str\n",
    "        The column in the df dataframe which contains datevalues\n",
    "    numeric_cols : list\n",
    "        The list of columns containing numeric values\n",
    "    modeling_granularity : list\n",
    "        The list of columns containg modeling granularity metrics\n",
    "    time_granularity : str, optional\n",
    "        The time granularity at which the dataset is grouped, by default \"weekly\". Possible values - 'weekly','daily'\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        Returns the dataframe where the missing values are imputed with the mean of the time granularity grouped data\n",
    "\n",
    "    Raises\n",
    "    ------\n",
    "    ValueError\n",
    "        if it fails to convert date column to datetime datatype\n",
    "    \"\"\"\n",
    "    if not isinstance(df[date_col],(np.datetime64)): # add more types if needed\n",
    "        try:\n",
    "            df[date_col] = pd.to_datetime(df[date_col])\n",
    "        except:\n",
    "            raise ValueError(\"Date column is not datetime. Failed to convert.\")\n",
    "    if time_granularity == \"weekly\":        \n",
    "        df[\"week_of_year\"] = df[date_col].dt.isocalendar().week  \n",
    "        model_level_col = [\"week_of_year\"]  \n",
    "    elif time_granularity==\"daily\":\n",
    "        df[\"Day\"] = df[date_col].dt.day\n",
    "        df[\"Month\"] = df[date_col].dt.month\n",
    "        model_level_col = [\"Day\",\"Month\"]  \n",
    "    \n",
    "    df_mean = df[modeling_granularity+model_level_col+numeric_cols].groupby(modeling_granularity+model_level_col).mean()\n",
    "    df_mean.columns = [\"New_\"+x if x in numeric_cols else x for x in df_mean.columns]\n",
    "    df_combine = df.merge(df_mean,on=modeling_granularity+model_level_col,how=\"left\")\n",
    "\n",
    "    for x in numeric_cols:\n",
    "        df_combine[x] = np.where(df_combine[x].isna(),df_combine[\"New_\"+x], df_combine[x])\n",
    "        df_combine = df_combine.drop([\"New_\"+x], axis = 1)\n",
    "    df_combine.drop(model_level_col, axis = 1, inplace = True)\n",
    "    # df_combine[date_col] = df_combine[date_col].astype(str)\n",
    "\n",
    "    return df_combine"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000020"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "532215ff-d206-4a3f-87a8-3204c281dce8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "name": "cell22",
    "collapsed": false
   },
   "source": "## 4.2. Function - Imputes the missing values\n\nThis function implements the missing value imputation methodology across various imputation methods. The function is called from UDF.",
   "id": "ce110000-1111-2222-3333-ffffff000021"
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a4e56ff2-fdd3-4cc1-8360-0fc602ed8c69",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "name": "cell23",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "def impute_missing_data(\n",
    "    df: pd.DataFrame,\n",
    "    col_name: list,\n",
    "    imputation_type: str,\n",
    "    arbitrary_value: int = 0,\n",
    "    window: int = None,\n",
    "    modeling_granularity: list = [],\n",
    "    time_granularity: str = None,\n",
    "    date_col: str = None,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Function to impute or fill the missing data with values based on the imputation type given\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        The raw dataset which contains the value for all variables\n",
    "    col_name : list\n",
    "        The list of names of the columns containing numerical values\n",
    "    imputation_type : str\n",
    "        The type of imputation based on which the missing values are filled\n",
    "    arbitrary_value : int, optional\n",
    "        The value which is used to fill missing values when imputation type is scalar, by default 0\n",
    "    window : int, optional\n",
    "        The size of the window used in the rolling mean and median methods, by default None\n",
    "    modeling_granularity : list, optional\n",
    "        The list of names of columns of modeling granularity values, by default []\n",
    "    time_granularity : str, optional\n",
    "        The time granularity values required for mean across years imputation type, by default None\n",
    "    date_col : str, optional\n",
    "        The name of the column containing date values, by default None\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        the input dataset after the missing values are imputed based on the given imputation type\n",
    "\n",
    "    Raises\n",
    "    ------\n",
    "    ValueError\n",
    "        if the window size is not provided for the rolling mean type\n",
    "    ValueError\n",
    "        if the window size is not provided for the rolling median type\n",
    "    ValueError\n",
    "        if the imputation type is not one of them in this function\n",
    "    \"\"\"\n",
    "    cleaned_data = df.copy()\n",
    "    if imputation_type==\"Mean\":\n",
    "        cleaned_data[col_name] = cleaned_data[col_name].fillna(cleaned_data[col_name].mean())\n",
    "    elif imputation_type==\"Median\":\n",
    "        cleaned_data[col_name] = cleaned_data[col_name].fillna(cleaned_data[col_name].median())\n",
    "    elif imputation_type==\"Scalar\":\n",
    "        cleaned_data[col_name] = cleaned_data[col_name].fillna(arbitrary_value)\n",
    "    elif imputation_type==\"Backward_fill\":\n",
    "        cleaned_data[col_name]= cleaned_data[col_name].fillna(method ='bfill').fillna(method ='ffill')\n",
    "    elif imputation_type==\"Forward_fill\":\n",
    "        cleaned_data[col_name]= cleaned_data[col_name].fillna(method ='ffill').fillna(method ='bfill')\n",
    "    elif imputation_type==\"Linear_Interpolation\":\n",
    "        cleaned_data[col_name]= cleaned_data[col_name].interpolate(method='linear').fillna(method ='ffill').fillna(method ='bfill')\n",
    "    elif imputation_type==\"Spline_Interpolation\":\n",
    "        cleaned_data[col_name]= cleaned_data[col_name].interpolate(option='spline').fillna(method ='ffill').fillna(method ='bfill')\n",
    "    elif imputation_type==\"Mode\":\n",
    "        cleaned_data[col_name]= cleaned_data[col_name].fillna(cleaned_data[col_name].mode().iloc[0])\n",
    "    elif imputation_type==\"Zero\":\n",
    "        cleaned_data[col_name]= cleaned_data[col_name].fillna(0)\n",
    "    elif imputation_type==\"Rolling_Mean\":\n",
    "        if window == None:\n",
    "            raise ValueError(\"Window Size not provided for rolling mean.\")\n",
    "        temp = cleaned_data[col_name].rolling(window, min_periods=1).mean()\n",
    "        cleaned_data[col_name] = np.where(cleaned_data[col_name].isna(),temp, cleaned_data[col_name])\n",
    "        cleaned_data[col_name] = cleaned_data[col_name].fillna(method = 'ffill').fillna(method ='bfill')\n",
    "    elif imputation_type==\"Rolling_Median\":\n",
    "        if window == None:\n",
    "            raise ValueError(\"Window Size not provided for rolling median.\")\n",
    "        temp = cleaned_data[col_name].rolling(window, min_periods=1).median()\n",
    "        cleaned_data[col_name] = np.where(cleaned_data[col_name].isna(),temp, cleaned_data[col_name])\n",
    "        cleaned_data[col_name] = cleaned_data[col_name].fillna(method = 'ffill').fillna(method ='bfill')\n",
    "    elif imputation_type == \"Mean_Across_Years\":\n",
    "        cleaned_data = mean_across_years(cleaned_data, date_col, col_name, modeling_granularity, time_granularity )\n",
    "        cleaned_data[col_name]= cleaned_data[col_name].fillna(method ='ffill').fillna(method ='bfill')\n",
    "    else:\n",
    "        raise ValueError(\"Incorrect imputation type\")\n",
    "    return cleaned_data"
   ],
   "id": "ce110000-1111-2222-3333-ffffff000022"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fbe195f6-1987-4b82-82aa-1e6e80871480",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "name": "cell24",
    "collapsed": false
   },
   "source": "# 5. Load Data\n\n- if missing date treatment is performed read the respective output\n- else read the harmonized data",
   "id": "ce110000-1111-2222-3333-ffffff000023"
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e520856e-45b6-4980-bf9f-3a272d8969ae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "name": "cell25",
    "language": "python"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Input file path: /dbfs/mnt/solutionsadls2_data/Baseline_Forecasts/processed/Data_Processing/Missing_Date_Treatment/Missing_Dates_Treatment_Results (2025-08-13-10-46-01)\n"
     ]
    }
   ],
   "source": "if app_config[\"data_processing\"][\"missing_dates_treatment_needed\"]:\n  # Read Missing date treatment output\n  df = session.table(\"ORANGE_ZONE_SBX_TA.PUBLIC.PROD_MISSING_DATE_TREATMENT_OUTPUT\")\nelse:\n  # Read Data Harmonization output\n  df = session.table(\"ORANGE_ZONE_SBX_TA.PUBLIC.PROD_ADS_STABLE_V4\")",
   "id": "ce110000-1111-2222-3333-ffffff000024"
  },
  {
   "cell_type": "code",
   "id": "ac65f12d-ce1a-4176-8ec4-64ffb508df5f",
   "metadata": {
    "language": "python",
    "name": "cell50",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "df",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "311e26fa-2799-4735-874e-97a3cc85a594",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "name": "cell26",
    "collapsed": false
   },
   "source": [
    "\n",
    "# 6. PySpark UDF Codes\n",
    "\n",
    "Codes related to applying pandas-UDF on pyspark-dataframe."
   ],
   "id": "ce110000-1111-2222-3333-ffffff000025"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8530d883-9eb2-4912-9d34-234581d7a3f0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "name": "cell27"
   },
   "source": [
    "## 6.1. UDF for Missing Value Treatment\n",
    "\n",
    "The function API to be used as a pandas-UDF."
   ],
   "id": "ce110000-1111-2222-3333-ffffff000026"
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ce396b11-9a39-4c65-a10e-47b670dc1f02",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "name": "cell28",
    "language": "python"
   },
   "outputs": [],
   "source": [
        "def missing_value_treatment_UDF(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"Function utilizing broadcasted information from the config file to treat the missing values in the input dataset\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    df : pd.DataFrame\n",
        "        the raw dataset which contains the value for all variables\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    pd.DataFrame\n",
        "        Returns the input dataframe after the missing values in them are treated\n",
        "    \"\"\"\n",
        "    df = df.sort_values(by=[broadcast_date_col],ascending=True)\n",
        "  \n",
        "    algo_params = broadcast_algo_params\n",
        "    modeling_granularity = broadcast_granularity\n",
        "    req_params = dict([x for x in broadcast_algo_params.items() if len(x[1]['cols'])>0])  \n",
        "    for algo in req_params.keys():\n",
        "        if algo in ['Rolling_Mean']:\n",
        "            window = int(algo_params[algo]['window'])\n",
        "            cols = algo_params[algo]['cols']\n",
        "            if algo_params[algo]['zero_as_missing_value'] == True:\n",
        "                df[cols] = df[cols].replace(0,np.nan)\n",
        "            df = impute_missing_data(df, cols, algo, window = window)\n",
        "        elif algo in ['Rolling_Median']:\n",
        "            window = int(algo_params[algo]['window'])\n",
        "            cols = algo_params[algo]['cols']\n",
        "            if algo_params[algo]['zero_as_missing_value'] == True:\n",
        "                df[cols] = df[cols].replace(0,np.nan)\n",
        "            df = impute_missing_data(df, cols, algo, window = window)\n",
        "        elif algo in ['Scalar']:\n",
        "            value = int(algo_params[algo]['value'])\n",
        "            cols = algo_params[algo]['cols']\n",
        "            if algo_params[algo]['zero_as_missing_value'] == True:\n",
        "                df[cols] = df[cols].replace(0,np.nan)\n",
        "            df = impute_missing_data(df, cols, algo, arbitrary_value = value)\n",
        "        elif algo in ['Forward_fill']:\n",
        "            cols = algo_params[algo]['cols']\n",
        "            df = impute_missing_data(df, cols, algo) \n",
        "        elif algo in ['Backward_fill']:\n",
        "            cols = algo_params[algo]['cols']\n",
        "            if algo_params[algo]['zero_as_missing_value'] == True:\n",
        "                df[cols] = df[cols].replace(0,np.nan)\n",
        "            df = impute_missing_data(df, cols, algo)\n",
        "        elif algo in ['Linear_Interpolation']:\n",
        "            cols = algo_params[algo]['cols']\n",
        "            if algo_params[algo]['zero_as_missing_value'] == True:\n",
        "                df[cols] = df[cols].replace(0,np.nan)\n",
        "            df = impute_missing_data(df, cols, algo)\n",
        "        elif algo in ['Spline_Interpolation']:\n",
        "            cols = algo_params[algo]['cols']\n",
        "            if algo_params[algo]['zero_as_missing_value'] == True:\n",
        "                df[cols] = df[cols].replace(0,np.nan)\n",
        "            df = impute_missing_data(df, cols, algo)\n",
        "        elif algo in ['Mean']:\n",
        "            cols = algo_params[algo]['cols']\n",
        "            if algo_params[algo]['zero_as_missing_value'] == True:\n",
        "                df[cols] = df[cols].replace(0,np.nan)\n",
        "            df = impute_missing_data(df, cols, algo)\n",
        "        elif algo in ['Median']:\n",
        "            cols = algo_params[algo]['cols']\n",
        "            if algo_params[algo]['zero_as_missing_value'] == True:\n",
        "                df[cols] = df[cols].replace(0,np.nan)\n",
        "            df = impute_missing_data(df, cols, algo)\n",
        "        elif algo in ['Mode']:\n",
        "            cols = algo_params[algo]['cols']\n",
        "            df = impute_missing_data(df, cols, algo)\n",
        "        elif algo in ['Zero']:\n",
        "            cols = algo_params[algo]['cols']\n",
        "            df = impute_missing_data(df, cols, algo)\n",
        "        elif algo in ['Mean_Across_Years']:\n",
        "            time_granularity = algo_params[algo]['time_granularity']\n",
        "            cols = algo_params[algo]['cols']\n",
        "            if algo_params[algo]['zero_as_missing_value'] == True:\n",
        "                df[cols] = df[cols].replace(0,np.nan)\n",
        "            df = impute_missing_data(df, cols, algo, modeling_granularity = modeling_granularity, time_granularity = time_granularity, date_col = broadcast_date_col.value)\n",
        "    # Convert numeric columns to native Python types to avoid Snowflake Decimal conversion errors\n",
        "    for col in df.select_dtypes(include=[np.integer, np.floating]).columns:\n",
        "         if np.issubdtype(df[col].dtype, np.integer):\n",
        "             df[col] = df[col].astype('int')  # or 'float' if decimals are expected\n",
        "         elif np.issubdtype(df[col].dtype, np.floating):\n",
        "             df[col] = df[col].astype('float')\n",
        "\n",
        "    return df"
      ],
   "id": "ce110000-1111-2222-3333-ffffff000027"
  },
  {
   "cell_type": "code",
   "id": "c6f10c54-0c5d-48ad-9293-31740698fce7",
   "metadata": {
    "language": "python",
    "name": "cell11"
   },
   "outputs": [],
   "source": "req_cols = modeling_granularity_conf + [ds_config]\nfor cols in [x[1]['cols'] for x in broadcast_algo_params.items() if len(x[1]['cols'])>0]:\n    req_cols.extend(cols)\nreq_cols",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "737586cc-3ae1-4ae9-a83d-191c707412d1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "name": "cell29",
    "collapsed": false
   },
   "source": [
    "## 6.2. Call UDF\n",
    "\n",
    "Applying the UDF on the input pyspark-dataframe."
   ],
   "id": "ce110000-1111-2222-3333-ffffff000028"
  },
  {
   "cell_type": "code",
   "id": "f69a02b0-e7f2-4008-b409-7ededa6e834f",
   "metadata": {
    "language": "python",
    "name": "cell54"
   },
   "outputs": [],
   "source": "modeling_granularity_conf",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "76b3a99d-dbad-4069-8721-db7da04e7e78",
   "metadata": {
    "language": "python",
    "name": "cell56",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "# Build expressions to count NULLs per column\nnull_counts = [\n    F.sum(F.when(F.col(c).is_null(), 1).otherwise(0)).alias(c)\n    for c in req_cols\n]\n\n# Aggregate once across all rows\nmissing_result_df = df.agg(*null_counts)\nmissing_result_df",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "8b55eb84-fbc0-4a2c-ad50-3e5c6f624092",
   "metadata": {
    "language": "python",
    "name": "cell57"
   },
   "outputs": [],
   "source": "df.count()",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "02fb60e6-e981-4398-80c8-874cb7d7d150",
   "metadata": {
    "language": "python",
    "name": "cell55",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "df = df.with_column(dv_config, F.col(dv_config).cast(\"float\"))",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "51864d57-7e18-4308-8d1e-295d32e04b9f",
   "metadata": {
    "language": "python",
    "name": "cell14"
   },
   "outputs": [],
   "source": "print(\"Modeling granularity ->\",modeling_granularity_conf)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aec8e54e-0d1f-4833-b19a-1526ed5ff7f1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "name": "cell30",
    "language": "python",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "results_s = (\n    df.select(req_cols)\n    .group_by(modeling_granularity_conf)\n    .applyInPandas(missing_value_treatment_UDF, output_schema=df.select(req_cols).schema)\n)",
   "id": "ce110000-1111-2222-3333-ffffff000029"
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9d35bb8c-aecb-4add-9f82-eb5868696451",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": null,
       "filterBlob": "{\"version\":1,\"filterGroups\":[],\"syncTimestamp\":1752552455112}",
       "queryPlanFiltersBlob": "[]",
       "tableResultIndex": 0
      }
     },
     "title": ""
    },
    "name": "cell31",
    "language": "python",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "results_s",
   "id": "ce110000-1111-2222-3333-ffffff000030"
  },
  {
   "cell_type": "code",
   "id": "0eb87ee5-2079-461e-a087-49371ef1f7dd",
   "metadata": {
    "language": "python",
    "name": "cell58",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "# Build expressions to count NULLs per column\nnull_counts = [\n    F.sum(F.when(F.col(c).is_null(), 1).otherwise(0)).alias(c)\n    for c in results_s.columns\n]\n\n# Aggregate once across all rows\nmissing_result_df = results_s.agg(*null_counts)\nmissing_result_df",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "cc4bc813-f5ab-4860-b233-b0e9d19706c3",
   "metadata": {
    "language": "python",
    "name": "cell59",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "results_s.count()",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "3f3add88-8d51-4ea8-9413-051c149341db",
   "metadata": {
    "language": "python",
    "name": "cell13"
   },
   "outputs": [],
   "source": "cleaned_cols = [c for c in req_cols if c not in modeling_granularity_conf + [ds_config]]",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "0bf17f00-582f-476d-b991-afb2b01afe0f",
   "metadata": {
    "language": "python",
    "name": "cell60"
   },
   "outputs": [],
   "source": "df_cleaned = df.select([c for c in df.columns if c not in cleaned_cols]).join(results_s, [*modeling_granularity_conf, ds_config], how = \"left\")\ndf_cleaned.count()",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "95385a6b-35f0-46ad-ad62-b8ca41efcedb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "name": "cell32",
    "collapsed": false
   },
   "source": [
    "# 7. Store Results\n",
    "\n",
    "Timestamp is added to the end of the file system because the notebook might be executed multiple times. This helps identify outputs from the most recent run."
   ],
   "id": "ce110000-1111-2222-3333-ffffff000031"
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3792f03d-15f3-4bb9-b797-80721a364cdb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "name": "cell34",
    "language": "python"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "File stored successfully.\n"
     ]
    }
   ],
   "source": "results_s_with_ts = df_cleaned.with_column(\"LOAD_TS\", F.current_timestamp())\nresults_s_with_ts.write.mode(\"overwrite\").save_as_table(output_table_name)\nprint(results_s_with_ts.count())",
   "id": "ce110000-1111-2222-3333-ffffff000033"
  },
  {
   "cell_type": "code",
   "id": "7585e8fc-efcb-423c-91e2-9c5a7e69ea6a",
   "metadata": {
    "language": "python",
    "name": "cell6",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "test_df = session.table(output_table_name)\nprint(\"All data ->\", test_df.count())\nlatest_ts = test_df.select(F.max(\"LOAD_TS\")).collect()[0][0]\nlatest_data = test_df.filter(F.col(\"LOAD_TS\") == F.lit(latest_ts))\nprint(\"Latest data ->\", latest_data.count())",
   "execution_count": null
  }
 ]
}
